{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTx54P9lxIp4pVBBcxAOgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/a-screen-pet/blob/main/implementing_rnn_and_lstms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KCdPGI8xvihO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        " keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        " keras.layers.SimpleRNN(20, return_sequences=True),\n",
        " keras.layers.SimpleRNN(1)\n",
        "])"
      ],
      "metadata": {
        "id": "6596r7FEvowj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Forecasting Several Time Steps Ahead"
      ],
      "metadata": {
        "id": "4enpuAFwvoy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deep rnn with  layer normalization\n"
      ],
      "metadata": {
        "id": "BK1jT_qbRBDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LayerNormalization\n",
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,\n",
        "                                                          activation=None)\n",
        "        self.layer_norm = LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
        "        if inputs is not None:\n",
        "            batch_size = tf.shape(inputs)[0]\n",
        "            dtype = inputs.dtype\n",
        "        return [tf.zeros([batch_size, self.state_size], dtype=dtype)]\n",
        "    def call(self, inputs, states):\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        return norm_outputs, [norm_outputs]\n"
      ],
      "metadata": {
        "id": "2yQMoAjlRBGQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
        "                     input_shape=[None, 1]),\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DHPhjm4UTkLS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RMSqdfIoTkW2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using one dimensional convolutional layers to process sequences\n",
        "#gru is grated recurrent unit"
      ],
      "metadata": {
        "id": "Vp2ur3slRBJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                        input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n"
      ],
      "metadata": {
        "id": "8UBXAV4fRBMw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wavenet\n",
        "# They stacked 1D convolutional layers, doubling the\n",
        "#dilation rate (how spread apart each neuron’s inputs are) at every layer: the first con‐\n",
        "#volutional layer gets a glimpse of just two time steps at a time, while the next one sees\n",
        "#four time steps (its receptive field is four time steps long\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                  activation=\"relu\", dilation_rate=rate))\n",
        "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0XwdpYZwWurQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedActivationUnit(keras.layers.Layer):\n",
        "    def __init__(self, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "    def call(self, inputs):\n",
        "        n_filters = inputs.shape[-1] // 2\n",
        "        linear_output = self.activation(inputs[..., :n_filters])\n",
        "        gate = keras.activations.sigmoid(inputs[..., n_filters:])\n",
        "        return self.activation(linear_output) * gate\n"
      ],
      "metadata": {
        "id": "0Us33LjBXwgZ",
        "outputId": "7d6ee437-c29a-48df-abd4-f2115351d11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    def call(self, inputs):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99d5swFLXwj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}